<!DOCTYPE html>
<html>
  <header>
    <title>Segment Anything Frontend Only Demo</title>
  </header>

  <body>
    <h1>Segment Anything in the Browser</h1>
    <div>
      <input title="Image from File" type="file" id="file-in" name="file-in" />
    </div>
    <div style="display: none">
      <img id="original-image" src="#" />
    </div>
    <h3>Status</h3>
    <span id="status">No image uploaded</span>
    <h3>Resized Image and Mask</h3>
    <canvas id="canvas"></canvas>
    <!-- import ONNXRuntime Web from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <script>
      var canvas = document.getElementById("canvas");
      const dimension = 1024;
      var image_embeddings;
      var imageImageData;

      async function handleClick(event) {
        const rect = canvas.getBoundingClientRect();
        const x = event.clientX - rect.left;
        const y = event.clientY - rect.top;

        console.log("Clicked position:", x, y);
        document.getElementById(
          "status"
        ).textContent = `Clicked on (${x}, ${y}). Downloading the decoder model if needed and generating mask...`;

        let context = canvas.getContext("2d");
        context.clearRect(0, 0, canvas.width, canvas.height);
        canvas.width = imageImageData.width;
        canvas.height = imageImageData.height;
        context.putImageData(imageImageData, 0, 0);
        context.fillStyle = "green";
        context.fillRect(x, y, 10, 10);

        const pointCoords = new ort.Tensor(
          new Float32Array([x, y, 0, 0]),
          [1, 2, 2]
        );
        const pointLabels = new ort.Tensor(new Float32Array([1, -1]), [1, 2]);
        // generate a (1, 1, 256, 256) tensor with all values set to 0
        const maskInput = new ort.Tensor(
          new Float32Array(256 * 256),
          [1, 1, 256, 256]
        );
        const hasMask = new ort.Tensor(new Float32Array([0]), [1]);
        const origianlImageSize = new ort.Tensor(
          new Float32Array([684, 1024]),
          [2]
        );
        try {
          const response = await fetch(
            "https://sam2-download.b-cdn.net/sam2_hiera_tiny.decoder.onnx",
            {
              method: "GET",
              mode: "cors",
              credentials: "omit",
              referrerPolicy: "no-referrer",
            }
          );
          const buffer = await response.arrayBuffer();
          const decodingSession = await ort.InferenceSession.create(buffer, {
            executionProviders: ["wasm"],
          });
          console.log("Decoder session", decodingSession);
          const decodingFeeds = {
            image_embeddings: image_embeddings,
            point_coords: pointCoords,
            point_labels: pointLabels,
            mask_input: maskInput,
            has_mask_input: hasMask,
            orig_im_size: origianlImageSize,
          };
          console.log("generating mask...");
          const start = Date.now();
          const results = await decodingSession.run(decodingFeeds);
          console.log("Generated mask:", results);
          const mask = results.masks;
          const maskImageData = mask.toImageData();
          context.globalAlpha = 0.5;
          // convert image data to image bitmap
          const imageBitmap = await createImageBitmap(maskImageData);
          context.drawImage(imageBitmap, 0, 0);
          const end = Date.now();
          console.log(`generating masks took ${(end - start) / 1000} seconds`);
          document.getElementById(
            "status"
          ).textContent = `Mask generated. Click on the image to generate new mask.`;
        } catch (error) {
          console.error("Error in handleClick:", error);
          document.getElementById(
            "status"
          ).textContent = `Error generating mask: ${error.message}`;
        }
      }

      async function handleImage(img) {
        try {
          console.log(status);
          document.getElementById(
            "status"
          ).textContent = `Uploaded image of size ${img.width}x${img.height}. Downloading the encoder model (~100 MB) if not cached and generating embedding. this will take a minute...`;
          console.log(`Uploaded image of size ${img.width}x${img.height}`);
          const scaleX = dimension / img.width;
          const scaleY = dimension / img.height;

          const resizedTensor = await ort.Tensor.fromImage(img, {
            resizedWidth: 1024,
            resizedHeight: 684,
          });
          const resizeImage = resizedTensor.toImageData();
          let imageDataTensor = await ort.Tensor.fromImage(resizeImage);
          imageImageData = imageDataTensor.toImageData();
          console.log("image data tensor:", imageDataTensor);

          // Presenting the images on dom
          canvas.width = imageImageData.width;
          canvas.height = imageImageData.height;
          let context = canvas.getContext("2d");
          context.putImageData(imageImageData, 0, 0);

          let tf_tensor = tf.tensor(imageDataTensor.data, imageDataTensor.dims);
          tf_tensor = tf_tensor.reshape([3, 684, 1024]);
          tf_tensor = tf_tensor.transpose([1, 2, 0]).mul(255);
          imageDataTensor = new ort.Tensor(
            "float32",
            tf_tensor.dataSync(),
            tf_tensor.shape
          );

          const response = await fetch(
            "https://sam2-download.b-cdn.net/sam2_hiera_tiny.encoder.onnx",
            {
              method: "GET",
              mode: "cors",
              credentials: "omit",
              referrerPolicy: "no-referrer",
            }
          );
          const buffer = await response.arrayBuffer();
          const session = await ort.InferenceSession.create(buffer, {
            executionProviders: ["wasm"],
          });
          console.log("Encoder Session", session);
          const feeds = { input_image: imageDataTensor };
          console.log("Computing image embedding; this will take a minute...");
          let start = Date.now();
          const results = await session.run(feeds);
          console.log("Encoding result:", results);
          image_embeddings = results.image_embeddings;
          let end = Date.now();
          let time_taken = (end - start) / 1000;
          console.log(`Computing image embedding took ${time_taken} seconds`);
          document.getElementById(
            "status"
          ).textContent = `Embedding generated in ${time_taken} seconds. Click on the image to generate mask.`;

          canvas.addEventListener("click", handleClick);
        } catch (error) {
          console.error("Error in handleImage:", error);
          document.getElementById(
            "status"
          ).textContent = `Error processing image: ${error.message}`;
        }
      }

      async function loadImage(fileReader) {
        var img = document.getElementById("original-image");
        img.onload = async () => await handleImage(img);
        img.src = fileReader.result;
      }

      // use an async context to call onnxruntime functions.
      async function main() {
        var img = document.getElementById("original-image");
        document.getElementById("file-in").onchange = function (evt) {
          let target = evt.target || window.event.src,
            files = target.files;
          if (FileReader && files && files.length) {
            var fileReader = new FileReader();
            fileReader.onload = async () => await loadImage(fileReader);
            fileReader.readAsDataURL(files[0]);
          }
        };
      }

      main().catch((error) => {
        console.error("Error in main function:", error);
        document.getElementById(
          "status"
        ).textContent = `Error initializing: ${error.message}`;
      });
    </script>
  </body>
</html>
